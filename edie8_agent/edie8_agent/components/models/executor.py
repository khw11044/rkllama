import sys
import os
import json
import random
import argparse
from typing import Any, List, Dict, Optional, Union

# rkllama_core ëª¨ë“ˆ ì¶”ê°€
sys.path.append(os.path.join(os.path.dirname(__file__), '../../../../../rkllama_function_calling'))

try:
    from rkllama_core import tool
except ImportError:
    print("rkllama_core import ì˜¤ë¥˜ - fallback to LangChain")
    from langchain.tools import tool

from edie8_agent.components.utils.rkllama_adapter import get_rkllama_agent
from edie8_agent.components.toolbox import ToolBox
from edie8_agent.components.prompts.agent_prompt import system_prompts, get_prompts
from edie8_agent.state_manager import get_state


class ActionExecutor:
    def __init__(
        self, 
        llm=None,
        add_prompt=None,
        tools: Optional[list] = None,
        tool_packages: Optional[list] = None,
        session_id: str = "default", 
        accumulate_chat_history: bool = True,
        verbose: bool = True,
        use_rkllama: bool = True,
    ):
        self.__session_id = session_id
        self.__verbose = verbose
        self.__accumulate_chat_history = accumulate_chat_history
        self.last_answer = None
        self.__agent_state = get_state()
        
        # rkllama_core ì‚¬ìš© ì—¬ë¶€ ê²°ì •
        if use_rkllama and 'rkllama_core' in sys.modules:
            self._init_rkllama_executor(add_prompt, tools, tool_packages)
        else:
            self._init_langchain_executor(llm, add_prompt, tools, tool_packages)
    
    def _init_rkllama_executor(self, add_prompt, tools, tool_packages):
        """rkllama_core ê¸°ë°˜ executor ì´ˆê¸°í™”"""
        try:
            # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ìƒì„±
            system_prompt = self._build_system_prompt(add_prompt)
            
            # rkllama agent ìƒì„±
            self.__agent = get_rkllama_agent(
                model_name="Qwen2.5-3B-Instruct_W8A8_G128_RK3588",
                temperature=0.6,
                system_prompt=system_prompt
            )
            
            # ë„êµ¬ ë°”ì¸ë”©
            if tool_packages or tools:
                all_tools = []
                if tool_packages:
                    for package in tool_packages:
                        all_tools.extend(self._extract_tools_from_package(package))
                if tools:
                    all_tools.extend(tools)
                
                self.__agent.bind_tools(all_tools)
            
            self.use_rkllama = True
            if self.__verbose:
                print("âœ… rkllama_core ê¸°ë°˜ ActionExecutor ì´ˆê¸°í™” ì™„ë£Œ")
                
        except Exception as e:
            print(f"âš ï¸ rkllama_core ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            print("ğŸ”„ LangChainìœ¼ë¡œ fallback...")
            self._init_langchain_executor(None, add_prompt, tools, tool_packages)
    
    def _init_langchain_executor(self, llm, add_prompt, tools, tool_packages):
        """ê¸°ì¡´ LangChain ê¸°ë°˜ executor ì´ˆê¸°í™” (fallback)"""
        self.__chat_history = []  
        self.__memory_key = "chat_history"
        self.__scratchpad = "agent_scratchpad"
        
        self.__llm = llm
        self.__prompts = self._get_prompts(add_prompt)
        self.__toolbox = self._get_tools(packages=tool_packages, tools=tools)
        self.__tools = self.__toolbox.get_tools()
        self.__llm_with_tools = self.__llm.bind_tools(self.__tools)
        self.__agent = self._get_agent()
        self.__executor = self._get_executor(verbose=self.__verbose)
        
        self.use_rkllama = False
        if self.__verbose:
            print("âœ… LangChain ê¸°ë°˜ ActionExecutor ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _build_system_prompt(self, add_prompt):
        """ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ êµ¬ì„±"""
        prompt_parts = []
        
        # ê¸°ë³¸ ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        for role, content in system_prompts:
            if role == "system":
                prompt_parts.append(content)
        
        # ë¡œë´‡ë³„ í”„ë¡¬í”„íŠ¸ ì¶”ê°€
        embodied_prompt = get_prompts()
        prompt_parts.append(str(embodied_prompt))
        
        # ì¶”ê°€ í”„ë¡¬í”„íŠ¸
        if add_prompt:
            for role, content in add_prompt:
                if role == "system":
                    prompt_parts.append(content)
        
        return "\n\n".join(prompt_parts)
    
    def _extract_tools_from_package(self, package):
        """íŒ¨í‚¤ì§€ì—ì„œ ë„êµ¬ í•¨ìˆ˜ë“¤ ì¶”ì¶œ"""
        tools = []
        for attr_name in dir(package):
            if not attr_name.startswith("_"):
                attr = getattr(package, attr_name)
                if hasattr(attr, '__call__') and hasattr(attr, 'name'):
                    tools.append(attr)
        return tools
        
    
    def _get_tools(
        self,
        packages: Optional[list],
        tools: Optional[list],
    ) -> ToolBox:
        """Create a ROSA tools object with the specified ROS version, tools, packages, and blacklist."""
        rosa_tools = ToolBox()
        if tools:
            rosa_tools.add_tools(tools)
        if packages:
            rosa_tools.add_packages(packages)
        return rosa_tools
    
    def _get_prompts(self, add_prompt) -> ChatPromptTemplate:
        """Create a chat prompt template from the system prompts and robot-specific prompts."""
        # Start with default system prompts
        prompts = system_prompts # action_executor_prompts
        embodied_prompt = get_prompts()
        
        prompts.append(embodied_prompt.as_message())

        template = ChatPromptTemplate.from_messages(
            prompts
            + 
            add_prompt
            +
            [
                MessagesPlaceholder(variable_name=self.__memory_key),
                ("user", "{state}"),
                MessagesPlaceholder(variable_name=self.__scratchpad),
            ]
        )
        return template
    
    def _get_agent(self):
        """Create and return an agent for processing user inputs and generating responses."""
        # agent = create_tool_calling_agent(self.__llm, self.__tools, self.__prompts)

        agent = (
            {
                "state": lambda x: x["state"],
                "agent_scratchpad": lambda x: format_to_openai_tool_messages(x["intermediate_steps"]),
                "chat_history": lambda x: x.get("chat_history", []),
            }
            | self.__prompts
            | self.__llm_with_tools
            | OpenAIToolsAgentOutputParser()
        )
        
        return agent
    

    def _get_executor(self, verbose: bool) -> AgentExecutor:
        """Create and return an executor for processing user inputs and generating responses."""
        executor = AgentExecutor(
            agent=self.__agent,
            tools=self.__tools,
            verbose=verbose,
            return_intermediate_steps=True,
            handle_parsing_errors=True,
            max_iterations=15,
            # max_execution_time=30,
        )

        return executor
    
    def invoke(self, state: str) -> Union[str, Dict]:
        """í†µí•©ëœ invoke ë©”ì„œë“œ - rkllama ë˜ëŠ” LangChain ì‚¬ìš©"""
        self.last_answer = None
        
        if self.use_rkllama:
            return self._invoke_rkllama(state)
        else:
            return self._invoke_langchain(state)
    
    def _invoke_rkllama(self, state: str) -> Dict:
        """rkllama_core ê¸°ë°˜ ì‹¤í–‰"""
        try:
            if self.__verbose:
                print(f"ğŸš€ rkllama Agent ì‹¤í–‰: {state[:50]}...")
            
            # rkllama agent í˜¸ì¶œ
            response = self.__agent.invoke(state)
            
            if self.__verbose:
                print(f"âœ… rkllama ì‘ë‹µ ì™„ë£Œ")
            
            return {
                "output": response,
                "agent_scratchpad": []  # rkllamaëŠ” ë‚´ë¶€ì ìœ¼ë¡œ tool calling ì²˜ë¦¬
            }
            
        except Exception as e:
            error_msg = f"rkllama ì‹¤í–‰ ì˜¤ë¥˜: {e}"
            print(f"âŒ {error_msg}")
            return {
                "output": error_msg,
                "agent_scratchpad": []
            }
    
    def _invoke_langchain(self, state: str) -> Dict:
        """ê¸°ì¡´ LangChain ê¸°ë°˜ ì‹¤í–‰"""
        try:
            result = self.__executor.invoke(
                {"state": state, "chat_history": self.__chat_history}
            )   
            
            output_text = result["output"]
            intermediate_steps = result.get("intermediate_steps", [])
            structured_steps = [
                {
                    "log": getattr(action, "log", ""),
                    "observation": observation
                }
                for action, observation in intermediate_steps
            ]
            
            self._record_chat_history(state, output_text)
            
            return {
                "output": output_text,
                "agent_scratchpad": structured_steps
            }
            
        except Exception as e:
            error_msg = f"LangChain ì‹¤í–‰ ì˜¤ë¥˜: {e}"
            print(f"âŒ {error_msg}")
            return {
                "output": error_msg,
                "agent_scratchpad": []
            }
    
    def chat(self, state: str) -> str:
        """ë‹¨ìˆœ ì±„íŒ…ìš© ë©”ì„œë“œ"""
        result = self.invoke(state)
        if isinstance(result, dict):
            return result.get("output", "")
        return result
    
    def invoke_iter(self, state: str) -> Dict:
        """ë°˜ë³µ ì‹¤í–‰ ë©”ì„œë“œ (ì¤‘ë‹¨ ì¡°ê±´ ì§€ì›)"""
        if self.use_rkllama:
            # rkllamaëŠ” ë‚´ë¶€ì ìœ¼ë¡œ ì™„ì „í•œ ì‹¤í–‰ì„ ìˆ˜í–‰í•˜ë¯€ë¡œ ì¼ë°˜ invoke ì‚¬ìš©
            return self.invoke(state)
        else:
            # LangChain ê¸°ë°˜ ë°˜ë³µ ì‹¤í–‰
            return self._invoke_langchain_iter(state)
    
    def _invoke_langchain_iter(self, state: str) -> Dict:
        """LangChain ê¸°ë°˜ ë°˜ë³µ ì‹¤í–‰"""
        structured_steps = []
        try:
            for step in self.__executor.iter({"state": state, "chat_history": self.__chat_history}):
                # ì¤‘ë‹¨ ì¡°ê±´ ì²´í¬
                if hasattr(self.__agent_state, 'command_case') and self.__agent_state.command_case == 0:
                    if self.__verbose:
                        print("âš ï¸ ì¤‘ê°„ ì¤‘ë‹¨ ì¡°ê±´ ê°ì§€! ì¦‰ì‹œ break")
                    return {
                        "output": "ìƒˆë¡œìš´ ëª…ë ¹ì´ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì¦‰ì‹œ í–‰ë™ì„ ë©ˆì¶¥ë‹ˆë‹¤.",
                        "agent_scratchpad": []
                    }
                
                if output := step.get("intermediate_step"):
                    action, value = output[0]
                    if self.__verbose:
                        print(f"ğŸ”§ ë„êµ¬ ì‚¬ìš©: {getattr(action, 'log', 'Unknown tool')}")
                    structured_step = {
                        "log": getattr(action, "log", ""),
                        "observation": value
                    }
                    structured_steps.append(structured_step)

            output_text = step.get("output", "")
            self._record_chat_history(state, output_text)
            
            return {
                "output": output_text,
                "agent_scratchpad": structured_steps
            }
            
        except Exception as e:
            error_msg = f"LangChain iter ì‹¤í–‰ ì˜¤ë¥˜: {e}"
            print(f"âŒ {error_msg}")
            return {
                "output": error_msg,
                "agent_scratchpad": structured_steps
            }

    def _record_chat_history(self, query: str, response: str):
        """ëŒ€í™” ê¸°ë¡ ì €ì¥"""
        if self.__accumulate_chat_history and hasattr(self, '_ActionExecutor__chat_history'):
            try:
                from langchain_core.messages import AIMessage, HumanMessage
                self.__chat_history.extend(
                    [HumanMessage(content=query), AIMessage(content=response)]
                )
            except ImportError:
                # LangChain ë¯¸ì‚¬ìš©ì‹œ ë‹¨ìˆœ ë¬¸ìì—´ë¡œ ì €ì¥
                if not hasattr(self, '_simple_history'):
                    self._simple_history = []
                self._simple_history.append({"user": query, "assistant": response})
    
    def clear_history(self):
        """ëŒ€í™” ê¸°ë¡ ì´ˆê¸°í™”"""
        if self.use_rkllama:
            if hasattr(self.__agent, 'clear_history'):
                self.__agent.clear_history()
        else:
            if hasattr(self, '_ActionExecutor__chat_history'):
                self.__chat_history = []
        
        if hasattr(self, '_simple_history'):
            self._simple_history = []
    
    def list_tools(self):
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ ëª©ë¡"""
        if self.use_rkllama:
            if hasattr(self.__agent, 'list_tools'):
                self.__agent.list_tools()
            else:
                print("ğŸ“‹ rkllama agentì˜ ë„êµ¬ ëª©ë¡ì„ í™•ì¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        else:
            if hasattr(self, '_ActionExecutor__tools'):
                print("ğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:")
                for tool in self.__tools:
                    print(f"  â€¢ {tool.name}: {getattr(tool, 'description', 'No description')}")
    
    

# === ë©”ì¸ ì‹¤í–‰ë¶€ ===
if __name__ == "__main__":
    

    verbose = True 
    
    chat_ID = str(random.randrange(1,100)) # input("IDë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”: ")
    
    model_name = "qwen3:1.7b" # qwen3:4b
    temperature = 0.6
    
    llm = ChatOllama(
                model=model_name, 
                temperature=temperature,
                # base_url = base_url
                )
    
    executor = ActionExecutor(llm=llm, session_id=chat_ID, verbose=verbose)

    
    
    cur_state = {
        'THOUGHT': 'ì£¼ë³€ì„ ë” ê¼¼ê¼¼íˆ ì‚´í´ë³¼ í•„ìš”ê°€ ìˆì–´ ë³´ì…ë‹ˆë‹¤. ë‹¤ì–‘í•œ ê°ì²´ì™€ ì¥ì†Œë“¤ì´ ëˆˆì— ë“¤ì–´ì˜¤ë„¤ìš”.',
        'EMOTION': 'í¥ë¯¸',
        'GOAL': 'ë‹¨ìˆœ ì´ë™í•˜ê¸° - ì£¼ë³€ì„ ë” ê¼¼ê¼¼íˆ ì‚´í´ë³´ê¸°',
        'PLAN': ['xì¶• ë°©í–¥ìœ¼ë¡œ 1m ì´ë™', 'yì¶• ë°©í–¥ìœ¼ë¡œ 1m ì´ë™'],
        'STEP': ['xì¶• ë°©í–¥ìœ¼ë¡œ 1m ì´ë™']
    }
    
    state_input = "\n".join([f"{k}: {v}" for k, v in cur_state.items()])
    
    final_answer = executor.invoke(state_input)
    
    # while True:
    #     # ìƒˆ í…ìŠ¤íŠ¸ ì…ë ¥ ì „ì— ì´ì „ì— ê³µìœ í–ˆë˜ ì¹´ë©”ë¼ í”¼ë“œ ì°½ì´ ì—´ë ¤ìˆë‹¤ë©´ ë‹«ìŠµë‹ˆë‹¤.
        
    #     # stream ì˜µì…˜ì— ë”°ë¼ í˜¸ì¶œ
    #     final_answer = executor.invoke(thought, goal, plan, step, thinking)
        
    #     print(final_answer)
